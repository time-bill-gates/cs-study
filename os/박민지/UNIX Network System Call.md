# 역사

- **초기(1960년대)**: 컴퓨터 간 통신 시스템이 없었고, 네트워크 시스템 콜의 개념은 존재하지 않음.
  - **특이점**: 프로세스 간 통신은 공유 메모리나 직접적인 하드웨어 연결을 통해서만 가능.

- **ARPANET와 TCP/IP 개발(1970~1980년대)**: TCP/IP 기반의 네트워크 프로토콜 개발로, 네트워크 시스템 호출 개념 도입. 프로세스 간 통신을 추상화하여 다양한 기기 간 연결을 지원.
  - **특이점**: TCP/IP 프로토콜과 소켓 인터페이스가 통합된 네트워크 시스템 콜이 개발되면서, 원격 기기 간 통신 가능.

- **BSD Unix와 소켓 API 등장(1983년)**: BSD Unix에서 **소켓(socket)** 개념을 도입한 네트워크 시스템 콜 등장. UNIX 시스템에서 네트워크 통신을 위한 표준 시스템 콜로 자리잡음.
  - **특이점**: 네트워크 프로그래밍을 위한 **소켓 인터페이스** 도입. 이를 통해 TCP/UDP를 사용하는 네트워크 통신이 간편해짐.

- **POSIX 표준화(1990년대)**: 네트워크 통신을 포함한 시스템 콜의 표준화. 소켓을 비롯한 네트워크 API가 POSIX 표준에 포함됨.
  - **특이점**: 다양한 운영체제 간 네트워크 시스템 콜의 일관성 확보.

- **IPv6 지원(2000년대)**: IPv4 주소 고갈 문제 해결을 위한 **IPv6 지원 시스템 콜** 추가. 기존 네트워크 시스템 콜 확장 및 새로운 콜 도입.
  - **특이점**: 기존 IPv4 시스템 콜과 호환성을 유지하면서, 대규모 주소 공간을 제공하는 IPv6 지원.

- **현대(2010년대 이후)**: 클라우드 컴퓨팅 및 컨테이너화된 환경에서 **고성능 네트워크 통신**을 지원하기 위한 **비동기 네트워크 시스템 콜**과 네트워크 성능 최적화를 위한 다양한 기술 발전.
  - **특이점**: 네트워크 성능을 극대화하기 위해 비동기 시스템 콜(`epoll`, `io_uring`)과 같은 고성능 네트워크 API 도입.

# 동작 과정

![[Screenshot 2024-10-15 at 7.19.22 PM.png]]

# 소켓 (Socket) 인터페이스

소켓 인터페이스는 BSD Unix에서 처음 도입된 네트워크 통신 시스템 콜의 표준 인터페이스로, 다양한 프로토콜(TCP, UDP 등)을 통해 네트워크 간 통신을 가능하게 합니다.

### 소켓의 동작 구조

1. **소켓 생성** (`socket()`):
   - 네트워크 통신을 위한 소켓을 생성합니다.
   - `int socket(int domain, int type, int protocol)`로 호출되며, 소켓을 사용할 프로토콜, 통신 방식(TCP/UDP) 등을 설정.

2. **주소 할당** (`bind()`):
   - 소켓에 IP 주소와 포트를 할당
   - 서버 소켓의 경우 필수적으로 IP와 포트

3. **연결 대기** (`listen()`):
   - 서버에서 클라이언트의 연결 요청을 기다리는 상태로 설정
   - 연결 요청이 있을 때까지 블로킹 모드로 동작

4. **연결 수락** (`accept()`):
   - 클라이언트의 연결 요청을 수락하고, 해당 클라이언트와의 통신을 위한 소켓을 반환

5. **데이터 송수신** (`send()`, `recv()`):
   - 생성된 소켓을 통해 데이터를 송수신
   - TCP 소켓의 경우 연결 상태에서 데이터를 주고받을 수 있으며, UDP 소켓은 비연결형 방식으로 작동.

6. **소켓 종료** (`close()`):
   - 소켓을 종료하고, 해당 자원을 해제

### 소켓 시스템 콜과 커널 호출

1. **`socket()`**:
   - `int socket(int domain, int type, int protocol)`을 호출하면 새로운 소켓이 생성되고, 커널에서 네트워크 프로토콜 스택에 맞는 소켓을 관리합니다.

2. **`bind()`**:
   - **`bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen)`**는 소켓에 특정 IP 주소와 포트를 할당합니다.
   - 커널에서는 소켓이 사용하는 네트워크 인터페이스와 해당 주소를 연동하여 관리

3. **`listen()`**:
   - 서버 소켓이 클라이언트의 연결 요청을 받을 준비가 되었음을 알립니다. 
   - **`listen(int sockfd, int backlog)`**로 호출하며, `backlog`는 대기할 최대 연결 수를 나타냅니다.

4. **`accept()`**:
   - 클라이언트로부터의 연결 요청을 받아들입니다. 
   - **`accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen)`**로 호출하며, 새로운 연결된 소켓을 반환합니다.

5. **`send()`, `recv()`**:
   - **`send(int sockfd, const void *buf, size_t len, int flags)`**는 데이터를 전송하고, **`recv(int sockfd, void *buf, size_t len, int flags)`**는 데이터를 수신합니다.

6. **`close()`**:
   - **`close(int sockfd)`**는 사용한 소켓을 닫고, 커널에서 해당 자원을 해제합니다.

---

# 비동기 네트워크 I/O

POSIX와 BSD Unix에서는 동기 I/O 모델이 일반적이지만, 고성능 서버와 같은 환경에서는 **비동기 네트워크 I/O**를 사용하여 성능을 최적화할 수 있습니다. 이는 네트워크 작업이 완료될 때까지 대기하지 않고, 다른 작업을 수행할 수 있게 해줍니다.

### 주요 비동기 네트워크 시스템 콜

### 1. **`select()`**

**`select()`**는 네트워크 소켓이나 파일 디스크립터에 대해 **다중 I/O 대기**를 수행하는 초기 시스템 콜 중 하나로, 여러 파일 디스크립터를 감시하여 **읽기, 쓰기, 오류 발생** 등의 이벤트가 발생하는지 확인합니다. 하나 이상의 파일 디스크립터에 대해 **동시 감시**할 수 있으며, 감지된 이벤트가 있으면 해당 디스크립터에 대해 작업을 수행할 수 있습니다.

#### 동작 방식:
- **`select()`**는 파일 디스크립터의 집합을 **읽기, 쓰기, 오류**로 나누어 감시할 수 있습니다.
  - `fd_set`이라는 구조체를 사용하여 감시할 파일 디스크립터 목록을 지정합니다.
- 특정 파일 디스크립터에서 읽기 또는 쓰기가 가능한 경우, 혹은 오류가 발생한 경우에 `select()`는 해당 파일 디스크립터를 **감지**합니다.
- 호출된 후, **타임아웃**이 발생하거나 파일 디스크립터에서 감지된 이벤트가 있을 때까지 **블로킹**됩니다.

#### 장점:
- **간단한 인터페이스**: 다양한 운영체제에서 널리 지원되며, 비교적 간단한 API로 동작합니다.
- **비차단 I/O 가능**: 타임아웃을 설정하여 **비동기 I/O**로 사용할 수 있습니다.

#### 단점:
- **성능 한계**: 파일 디스크립터 수가 많아질수록, 모든 디스크립터를 반복적으로 확인해야 하므로 성능이 저하됩니다.
  - `select()`는 감시할 수 있는 파일 디스크립터의 수가 제한되어 있습니다(리눅스에서는 보통 1024개로 제한).
- **폴링(Polling) 오버헤드**: 이벤트가 발생하지 않는 파일 디스크립터를 계속해서 검사해야 하므로, 불필요한 자원 낭비가 발생할 수 있습니다.

#### 사용 예시:
```c
fd_set readfds;
FD_ZERO(&readfds);
FD_SET(sockfd, &readfds);

int activity = select(sockfd + 1, &readfds, NULL, NULL, &timeout);

if (activity > 0) {
    if (FD_ISSET(sockfd, &readfds)) {
        // 소켓에서 읽을 데이터가 있습니다.
    }
}
```

### 2. **`poll()`**

**`poll()`**는 **`select()`**와 유사하지만, 몇 가지 개선 사항이 있습니다. 파일 디스크립터 수에 제한이 없고, 감시할 파일 디스크립터를 리스트로 관리하며, 파일 디스크립터가 이벤트를 감지하는 방식이 개선되었습니다. `poll()`은 여러 소켓이나 파일 디스크립터에서 **동시 이벤트 감지**를 수행하는 데 사용됩니다.

#### 동작 방식:
- **`poll()`**은 `pollfd` 구조체 배열을 사용하여 여러 파일 디스크립터를 감시합니다. 각 파일 디스크립터에 대해 감시할 **읽기, 쓰기, 오류** 등의 이벤트를 설정할 수 있습니다.
- **타임아웃**을 지정하여 **블로킹** 동작을 제어할 수 있으며, 지정된 이벤트가 발생하거나 타임아웃이 발생할 때까지 대기합니다.
- 이벤트가 발생한 파일 디스크립터만 반환하여, 이후 필요에 따라 처리할 수 있습니다.

#### 장점:
- **파일 디스크립터 수 제한 없음**: `select()`는 파일 디스크립터 수에 제한이 있지만, `poll()`은 감시할 수 있는 파일 디스크립터 수에 제한이 없습니다.
- **상태 기반 이벤트 처리**: `poll()`은 이벤트 발생 여부에 따라 상태를 갱신하여, 이벤트가 발생한 파일 디스크립터만 반환합니다.

#### 단점:
- **성능 문제**: 파일 디스크립터가 매우 많은 경우, 여전히 반복적으로 파일 디스크립터 상태를 확인해야 하므로 성능 저하가 발생할 수 있습니다.
- **동적 메모리 할당**: `poll()`은 파일 디스크립터 목록을 동적으로 할당해야 하므로, 추가적인 메모리 관리가 필요할 수 있습니다.

#### 사용 예시:
```c
struct pollfd fds[2];
fds[0].fd = sockfd;
fds[0].events = POLLIN; // 읽기 가능 여부 감시
fds[1].fd = otherfd;
fds[1].events = POLLOUT; // 쓰기 가능 여부 감시

int ret = poll(fds, 2, timeout);

if (ret > 0) {
    if (fds[0].revents & POLLIN) {
        // sockfd에서 읽을 데이터가 있습니다.
    }
    if (fds[1].revents & POLLOUT) {
        // otherfd에서 쓸 수 있습니다.
    }
}
```

### 3. **`epoll()`**

**`epoll()`**은 리눅스에서 제공하는 고성능 비동기 I/O 시스템 콜로, **이벤트 기반**으로 파일 디스크립터를 감시하는 기능을 제공합니다. **`select()`**나 **`poll()`**의 성능 문제를 해결하기 위해 도입된 시스템 콜로, 많은 소켓이나 파일 디스크립터를 관리할 때 뛰어난 성능을 발휘합니다. 특히 **대규모 네트워크 서버**에서 많이 사용됩니다.

#### 동작 방식:
- **`epoll_create()`**를 호출하여 **epoll 인스턴스**를 생성하고, **파일 디스크립터**를 감시할 수 있는 공간을 마련합니다.
- **`epoll_ctl()`**을 사용하여 감시할 파일 디스크립터를 **등록, 수정, 삭제**할 수 있습니다.
- **`epoll_wait()`**를 사용하여 등록된 파일 디스크립터 중 이벤트가 발생한 디스크립터만을 반환하여 처리합니다.
- 이벤트 기반으로 동작하여, 이벤트가 발생한 디스크립터에 대해서만 처리가 이루어지므로, **불필요한 폴링**을 피할 수 있습니다.

#### 장점:
- **높은 성능**: 많은 수의 파일 디스크립터를 관리할 때 `select()`나 `poll()`보다 훨씬 효율적입니다.
- **이벤트 기반**: 이벤트가 발생한 디스크립터만 처리하므로, 대기 시간이나 불필요한 자원 사용을 줄일 수 있습니다.
- **비동기 I/O 지원**: 대규모 동시 접속 처리가 요구되는 서버(예: 대형 웹 서버)에서 성능 최적화에 탁월한 선택입니다.

#### 단점:
- **리눅스 전용**: `epoll()`은 리눅스 고유의 시스템 콜이므로, 이식성 측면에서는 제한이 있을 수 있습니다.
- **복잡한 인터페이스**: 설정과 제어가 `select()`나 `poll()`보다 복잡하며, 추가적인 관리가 필요합니다.

#### 사용 예시:
```c
int epoll_fd = epoll_create1(0); // epoll 인스턴스 생성

struct epoll_event ev, events[MAX_EVENTS];
ev.events = EPOLLIN;
ev.data.fd = sockfd;
epoll_ctl(epoll_fd, EPOLL_CTL_ADD, sockfd, &ev); // sockfd 등록

int nfds = epoll_wait(epoll_fd, events, MAX_EVENTS, timeout); // 이벤트 대기

for (int i = 0; i < nfds; i++) {
    if (events[i].events & EPOLLIN) {
        // 읽을 수 있는 데이터가 있습니다.
    }
}
```

### 4. **`io_uring()`**

**`io_uring()`**은 리눅스 커널 5.x 버전에서 도입된 최신 **고성능 비동기 I/O 시스템 콜**입니다. **낮은 오버헤드**로 대규모 I/O 작업을 비동기적으로 처리할 수 있도록 설계되었습니다. `io_uring`은 기존의 `epoll()` 등과 비교해 **더 나은 성능**을 제공하며, 특히 대규모 I/O를 요구하는 고성능 서버에서 널리 사용될 수 있습니다.

#### 동작 방식:
- **메모리 매핑 링 버퍼**: `io_uring()`은 커널과 사용자 공간 사이에 **공유 메모리 링 버퍼**를 설정합니다. 이를 통해 커널과 애플리케이션 간의 **시스템 콜 전환 오버헤드**를 줄일 수 있습니다.
- **제출 큐와 완료 큐**: 애플리케이션은 I/O 요청을 **제출 큐**에 추가하고, 커널은 이 큐를 처리하여 완료된 작업을 **완료 큐**에 기록합니다.
  - 이를 통해 애플리케이션은 I/O 작업이 완료되었을 때 완료 큐에서 결과를 확인하고, 시스템 콜 호출 없이도 비동기 I/O를 효율적으로 처리할 수 있습니다.
- **비동기 I/O**를 지원하면서도 **동기식 I/O**와 비슷한 방식으로 사용할 수 있는 유연성을 제공합니다.

#### 장점:
- **낮

은 오버헤드**: 커널과의 상호작용에서 발생하는 **시스템 콜 전환 오버헤드**가 거의 없습니다.
- **효율적 자원 관리**: 대규모 I/O 작업을 처리할 때 효율적이며, 네트워크와 디스크 I/O 모두에서 성능이 크게 향상됩니다.
- **멀티큐 시스템**: 커널과의 I/O 작업이 **제출 큐**와 **완료 큐**를 통해 처리되므로, 커널 내의 스케줄링이 효율적입니다.

#### 단점:
- **상대적으로 새로운 기술**: `io_uring()`은 커널 5.x에서 도입되었기 때문에, 구형 리눅스 커널에서는 지원되지 않을 수 있습니다.
- **복잡한 사용 방식**: 성능 향상을 위해 링 버퍼와 큐를 직접 관리해야 하므로, 사용법이 상대적으로 복잡할 수 있습니다.

#### 사용 예시:
```c
struct io_uring ring;
io_uring_queue_init(32, &ring, 0); // io_uring 초기화

struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);
io_uring_prep_read(sqe, sockfd, buffer, sizeof(buffer), 0); // 읽기 준비

io_uring_submit(&ring); // I/O 작업 제출

struct io_uring_cqe *cqe;
io_uring_wait_cqe(&ring, &cqe); // 완료된 작업 대기

if (cqe->res >= 0) {
    // 성공적으로 데이터 읽음
}

io_uring_cqe_seen(&ring, cqe); // 완료된 작업 처리
```

---

# 주요 네트워크 시스템 콜

### 1. **소켓 생성 (`socket`)**

#### 시스템 호출: `socket()`
- **정의**: `int socket(int domain, int type, int protocol)`
- 소켓을 생성하는 시스템 호출입니다.
- `domain`: 통신 영역(IPv4: `AF_INET`, IPv6: `AF_INET6` 등)
- `type`: 소켓 타입(TCP: `SOCK_STREAM`, UDP: `SOCK_DGRAM` 등)
- `protocol`: 사용 프로토콜(TCP, UDP 등)

#### 커널 호출: `sock_create()`
- 커널에서 소켓을 생성하는 함수입니다. 사용자 모드에서 호출된 `socket()`이 이 커널 함수를 호출합니다.

---

### 2. **소켓에 주소 할당 (`bind`)**

#### 시스템 호출: `bind()`
- **정의**: `int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen)`
- 소켓에 특정 IP 주소와 포트를 할당하는 시스템 호출입니다. 서버 소켓에서 필수적으로 사용됩니다.

#### 커널 호출: `sock_bind()`
- 커널에서 소켓에 주소를 연결하는 함수입니다. 소켓이 특정 네트워크 인터페이스와 연결되도록 설정합니다.

---

### 3. **연결 대기 (`listen`)**

#### 시스템 호출: `listen()`
- **정의**: `int listen(int sockfd, int backlog)`
- 서버 소켓이 클라이언트의 연결 요청을 대기할 수 있게 설정하는 시스템 호출입니다.

#### 커널 호출: `sock_listen()`
- 커널에서 연결 요청을 처리하기 위해 소켓을 대기 상태로 전환하는 함수입니다.

---

### 4. **연결 수락 (`accept`)**

#### 시스템 호출: `accept()`
- **정의**: `int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen)`
- 클라이언트 연결 요청을 수락하고, 새로운 연결된 소켓을 반환하는 시스템 호출입니다.

#### 커널 호출: `sock_accept()`
- 커널에서 클라이언트 연결을 처리하고, 해당 연결에 대한 새로운 소켓을 생성하는 함수입니다.

---

### 5.**데이터 송수신 (`send`, `recv`)**

#### 시스템 호출: `send()`, `recv()`
- **정의**:
  - `send()`: `ssize_t send(int sockfd, const void *buf, size_t len, int flags)`
  - `recv()`: `ssize_t recv(int sockfd, void *buf, size_t len, int flags)`
- 소켓을 통해 데이터를 전송(`send`)하거나 수신(`recv`)하는 시스템 호출입니다.

#### 커널 호출: `sock_sendmsg()`, `sock_recvmsg()`
- 커널에서 소켓을 통해 데이터를 송수신하는 함수입니다.


# References
- https://smileostrich.tistory.com/entry/What-is-IOuring-Inside-IOuring